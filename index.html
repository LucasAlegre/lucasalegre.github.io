<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="">
<meta name="msvalidate.01" content="">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Lucas N. Alegre</title>
    <meta name="author" content="Lucas N. Alegre">
    <meta name="description" content="Lucas N. Alegre's web page.
">
    <meta name="keywords" content="reinforcement learning, transfer learning, multi-objective RL">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://lucasalegre.github.io/">

    <!-- Dark Mode -->
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <!-- Social Icons -->
          <div class="navbar-brand social">
            <a href="mailto:%6C%6E%61%6C%65%67%72%65@%69%6E%66.%75%66%72%67%73.%62%72" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0001-5465-4390" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="fab fa-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=YZnEeJUAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="http://lattes.cnpq.br/8350276337206370" target="_blank" title="Lattes" rel="external nofollow noopener"><i class="ai ai-lattes"></i></a>
            <a href="https://github.com/LucasAlegre" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/lucas-alegre-b80628127" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://bsky.app/profile/lnalegre.bsky.social" title="BlueSky" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-bluesky"></i></a>
            <a href="https://twitter.com/lnalegre" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a>
            

          </div>
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">Home<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Open-Source</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">Teaching</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/assets/pdf/CV.pdf">CV</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Lucas</span> N. Alegre
          </h1>
          <p class="desc"><a href="https://www.inf.ufrgs.br/site/en/" rel="external nofollow noopener" target="_blank">Institute of Informatics</a> at <a href="http://www.ufrgs.br/english/home" rel="external nofollow noopener" target="_blank">Federal University of Rio Grande do Sul (UFRGS)</a> <br>lnalegre@inf.ufrgs.br</p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

            <div class="address" style="font-size: 15px">
              <p>INF - UFRGS</p> <p>Porto Alegre - RS, Brasil</p>

            </div>
          </div>

          <div class="clearfix">
            <hr>

<p>I am a Professor at the <a href="https://www.inf.ufrgs.br/site/en/" rel="external nofollow noopener" target="_blank">Institute of Informatics</a> at the <a href="http://www.ufrgs.br/english/home" rel="external nofollow noopener" target="_blank">Federal University of Rio Grande do Sul (UFRGS)</a>. I hold a Ph.D. in Computer Science from this same university, where I was advised by Prof. <a href="https://www.inf.ufrgs.br/~bazzan/" rel="external nofollow noopener" target="_blank">Ana Bazzan</a> and Prof. <a href="https://people.cs.umass.edu/~bsilva" rel="external nofollow noopener" target="_blank">Bruno C. da Silva</a>.
Part of my Ph.D. was done in the <a href="https://ai.vub.ac.be/" rel="external nofollow noopener" target="_blank">AI Lab</a> at the <a href="https://www.vub.be/" rel="external nofollow noopener" target="_blank">Vrije Universiteit Brussel (VUB)</a> under the supervision of Prof. <a href="https://ai.vub.ac.be/team/ann-nowe/" rel="external nofollow noopener" target="_blank">Ann Nowé</a>.</p>

<p>I am also part of the <a href="https://farama.org/team" rel="external nofollow noopener" target="_blank">Farama Foundation</a>, a nonprofit organization that maintains the largest open-source RL libraries in the world.</p>

<p>I completed my B.Sc. in Computer Science <i>cum laude</i> at the <a href="http://www.ufrgs.br/english/home" rel="external nofollow noopener" target="_blank">Federal University of Rio Grande do Sul</a> in 2020.
My undergraduate thesis, advised by Prof. <a href="https://people.cs.umass.edu/~bsilva" rel="external nofollow noopener" target="_blank">Bruno C. da Silva</a>, tackled the problem of reinforcement learning in continuous non-stationary environments.</p>

<!-- In Winter of 2020, I worked as an intern researcher at the <a href="https://www.tu.berlin/en/">Technische Universität Berlin</a>, under supervision of Prof. <a href="https://www.tu.berlin/vsp/ueber-uns/fachgebietsleitung">Kai Nagel</a>. -->

<hr>

<p><i>
My main research interests are in reinforcement learning (RL) and its use to empower artificial intelligence (AI) agents to solve real-world problems.
</i></p>

<p><i><mark style="background: rgba(0, 118, 223, 0.1)">In my <a href="https://lume.ufrgs.br/handle/10183/290816" rel="external nofollow noopener" target="_blank">Ph.D. Thesis</a>, I tackled the problem of <strong>how to design principled sample-efficient RL algorithms capable of learning multiple behaviors that can be combined to solve multi-task and multi-objective problems.</strong></mark></i></p>
<hr>

<!-- Write your biography here. Tell the world about yourself. 
Link to your favorite [subreddit](http://reddit.com). 
You can put a picture in, too. 
The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.
 -->
<!-- Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

          </div>

          <!-- News -->
          

          <!-- Latest posts -->
          

          <!-- Selected papers -->
          <h2><a href="/publications/" style="color: inherit;">Selected Publications</a></h2> (see <a href="/publications/">full list</a>)
          <div class="publications">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/ok.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ok.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://neurips.cc" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="Alegre+2025" class="col-sm-8">
        <!-- Title -->
        <div class="title">Constructing an Optimal Behavior Basis for the Option Keyboard</div>
        <!-- Author -->
        <div class="author">
        

        <em>Lucas N. Alegre</em>, <a href="https://www.inf.ufrgs.br/~bazzan" rel="external nofollow noopener" target="_blank">Ana L. C. Bazzan</a>, <a href="https://sites.google.com/view/andrebarreto/about" rel="external nofollow noopener" target="_blank">André Barreto</a>, and <a href="https://people.cs.umass.edu/~bsilva/" rel="external nofollow noopener" target="_blank">Bruno C. da Silva</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the Thirty-ninth Conference on Neural Information Processing Systems (NeurIPS)</em>, 2025
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);">  </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2505.00787" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="/assets/pdf/okb.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Multi-task reinforcement learning aims to quickly identify solutions for new tasks with minimal or no additional interaction with the environment. Generalized Policy Improvement (GPI) addresses this by combining a set of base policies to produce a new one that is at least as good – though not necessarily optimal – as any individual base policy. Optimality can be ensured, particularly in the linear-reward case, via techniques that compute a Convex Coverage Set (CCS). However, these are computationally expensive and do not scale to complex domains. The Option Keyboard (OK) improves upon GPI by producing policies that are at least as good – and often better. It achieves this through a learned meta-policy that dynamically combines base policies. However, its performance critically depends on the choice of base policies. This raises a key question: is there an optimal set of base policies – an optimal behavior basis – that enables zero-shot identification of optimal solutions for any linear tasks? We solve this open problem by introducing a novel method that efficiently constructs such an optimal behavior basis. We show that it significantly reduces the number of base policies needed to ensure optimality in new tasks. We also prove that it is strictly more expressive than a CCS, enabling particular classes of non-linear tasks to be solved optimally. We empirically evaluate our technique in challenging domains and show that it outperforms state-of-the-art approaches, increasingly so as task complexity increases.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Alegre+2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Constructing an Optimal Behavior Basis for the Option Keyboard}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alegre, Lucas N. and Bazzan, Ana L. C. and Barreto, André and {da Silva}, Bruno C.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Thirty-ninth Conference on Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{San Diego, USA}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/amorgif.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="amorgif.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://s2025.siggraph.org" rel="external nofollow noopener" target="_blank">SIGGRAPH</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="Alegre+2025siggraph" class="col-sm-8">
        <!-- Title -->
        <div class="title">AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning</div>
        <!-- Author -->
        <div class="author">
        

        <em>Lucas N. Alegre</em>, <a href="https://aserifi.github.io" rel="external nofollow noopener" target="_blank">Agon Serifi</a>, Ruben Grandia, David Müller, Espen Knoop, and <a href="https://www.baecher.info" rel="external nofollow noopener" target="_blank">Moritz Bächer</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 52nd Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)</em>, 2025
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);">  </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/pdf/2505.23708" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://la.disneyresearch.com/publication/amor-adaptive-character-control-through-multi-objective-reinforcement-learning/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Reinforcement learning (RL) has significantly advanced the control of physics-based and robotic characters that track kinematic reference motion. However, methods typically rely on a weighted sum of conflicting reward functions, requiring extensive tuning to achieve a desired behavior. Due to the computational cost of RL, this iterative process is a tedious, time-intensive task. Furthermore, for robotics applications, the weights need to be chosen such that the policy performs well in the real world, despite inevitable sim-to-real gaps. To address these challenges, we propose a multi-objective reinforcement learning framework that trains a single policy conditioned on a set of weights, spanning the Pareto front of reward trade-offs. Within this framework, weights can be selected and tuned after training, significantly speeding up iteration time. We demonstrate how this improved workflow can be used to perform highly dynamic motions with a robot character. Moreover, we explore how weight-conditioned policies can be leveraged in hierarchical settings, using a high-level policy to dynamically select weights according to the current task. We show that the multi-objective policy encodes a diverse spectrum of behaviors, facilitating efficient adaptation to novel tasks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Alegre+2025siggraph</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alegre, Lucas N. and Serifi, Agon and Grandia, Ruben and Müller, David and Knoop, Espen and Bächer, Moritz}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{AMOR}: Adaptive Character Control through Multi-Objective Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 52nd Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Vancouver, Canada}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/sc.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="sc.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://jmlr.org/tmlr" rel="external nofollow noopener" target="_blank">TMLR</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="Bagot+2025" class="col-sm-8">
        <!-- Title -->
        <div class="title">Successor Clusters: A Behavior Basis for Unsupervised Zero-Shot Reinforcement Learning</div>
        <!-- Author -->
        <div class="author">
        

        Louis Bagot, <em>Lucas N. Alegre</em>, Steven Latre, Kevin Mets, and Bruno Castro Silva</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Transactions on Machine Learning Research</em>, 2025
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);">  </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://openreview.net/forum?id=UB22Tt3sfF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this work, we introduce Successor Clusters (SCs), a novel method for tackling unsupervised zero-shot reinforcement learning (RL) problems. The goal in this setting is to directly identify policies capable of optimizing any given reward functions without requiring further learning after an initial reward-free training phase. Existing state-of-the-art techniques leverage Successor Features (SFs)—functions capable of characterizing a policy’s expected discounted sum of a set of d reward features. Importantly, however, the performance of existing techniques depends critically on how well the reward features enable arbitrary reward functions of interest to be linearly approximated. We introduce a novel and principled approach for constructing reward features and prove that they allow for any Lipschitz reward functions to be approximated arbitrarily well. Furthermore, we mathematically derive upper bounds on the corresponding approximation errors. Our method constructs features by clustering the state space via a novel distance metric quantifying the minimal expected number of timesteps needed to transition between any state pairs. Building on these theoretical contributions, we introduce Successor Clusters (SCs), a variant of the successor features framework capable of predicting the time spent by a policy in different regions of the state space. We demonstrate that, after a pre-training phase, our method can approximate and maximize any new reward functions in a zero-shot manner. Importantly, we also formally show that as the number and quality of clusters increase, the set of policies induced by Successor Clusters converges to a set containing the optimal policy for any new task. Moreover, we show that our technique naturally produces interpretable features, enabling applications such as visualizing the sequence of state regions an agent is likely to visit while solving a task. Finally, we empirically demonstrate that our method outperforms stateof-the-art SF-based competitors in challenging continuous control benchmarks, achieving superior zero-shot performance and lower reward approximation error.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Bagot+2025</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Successor Clusters: A Behavior Basis for Unsupervised Zero-Shot Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bagot, Louis and Alegre, Lucas N. and Latre, Steven and Mets, Kevin and da Silva, Bruno Castro}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2835-8856}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/h-gpi.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="h-gpi.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://neurips.cc" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="Alegre+2023neurips" class="col-sm-8">
        <!-- Title -->
        <div class="title">Multi-Step Generalized Policy Improvement by Leveraging Approximate Models</div>
        <!-- Author -->
        <div class="author">
        

        <em>Lucas N. Alegre</em>, <a href="https://www.inf.ufrgs.br/~bazzan" rel="external nofollow noopener" target="_blank">Ana L. C. Bazzan</a>, <a href="https://scholar.google.com/citations?user=LH5QKbgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ann Nowé</a>, and <a href="https://people.cs.umass.edu/~bsilva/" rel="external nofollow noopener" target="_blank">Bruno C. da Silva</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)</em>, 2023
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);">  </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://openreview.net/pdf?id=KFj0Q1EXvU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="/assets/pdf/h-gpi.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We introduce a principled method for performing zero-shot transfer in reinforcement learning (RL) by exploiting approximate models of the environment. Zero-shot transfer in RL has been investigated by leveraging methods rooted in generalized policy improvement (GPI) and successor features (SFs). Although computationally efficient, these methods are model-free: they analyze a library of policies—each solving a particular task—and identify which action the agent should take. We investigate the more general setting where, in addition to a library of policies, the agent has access to an approximate environment model. Even though model-based RL algorithms can identify near-optimal policies, they are typically computationally intensive. We introduce h-GPI, a multi-step extension of GPI that interpolates between these extremes—standard model-free GPI and full model-based planning—as a function of a parameter, h, regulating the amount of time the agent has to reason. We prove that h-GPI’s performance lower bound is strictly better than GPI’s, and show that h-GPI generally outperforms GPI as h increases. Furthermore, we prove that as h increases, h-GPI’s performance becomes arbitrarily less susceptible to sub-optimality in the agent’s policy library. Finally, we introduce novel bounds characterizing the gains achievable by h-GPI as a function of approximation errors in both the agent’s policy library and its (possibly learned) model. These bounds strictly generalize those known in the literature. We evaluate h-GPI on challenging tabular and continuous-state problems under value function approximation and show that it consistently outperforms GPI and state-of-the-art competing methods under various levels of approximation errors.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Alegre+2023neurips</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alegre, Lucas N. and Bazzan, Ana L. C. and Now{\'e}, Ann and {da Silva}, Bruno C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-Step Generalized Policy Improvement by Leveraging Approximate Models}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{36}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{New Orleans, USA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/mo-gym-logo.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="mo-gym-logo.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://neurips.cc" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="Felten+2023" class="col-sm-8">
        <!-- Title -->
        <div class="title">A Toolkit for Reliable Benchmarking and Research in Multi-Objective Reinforcement Learning</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://ffelten.github.io/" rel="external nofollow noopener" target="_blank">Florian Felten*</a>, <em>Lucas N. Alegre*</em>, <a href="https://scholar.google.com/citations?user=LH5QKbgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ann Nowé</a>, <a href="https://www.inf.ufrgs.br/~bazzan" rel="external nofollow noopener" target="_blank">Ana L. C. Bazzan</a>, El-Ghazali Talbi, <a href="https://scholar.google.com/citations?user=XDRF0uwAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Grégoire Danoy</a>, and <a href="https://people.cs.umass.edu/~bsilva/" rel="external nofollow noopener" target="_blank">Bruno C. da Silva</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks</em>, 2023
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);">  </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://openreview.net/pdf?id=jfwRLudQyj" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/Farama-Foundation/MO-Gymnasium" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="/assets/pdf/toolkit.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Multi-objective reinforcement learning algorithms (MORL) extend standard reinforcement learning (RL) to scenarios where agents must optimize multiple—potentially conflicting—objectives, each represented by a distinct reward function. To facilitate and accelerate research and benchmarking in multi-objective RL problems, we introduce a comprehensive collection of software libraries that includes: (i) MO-Gymnasium, an easy-to-use and flexible API enabling the rapid construction of novel MORL environments. It also includes more than 20 environments under this API. This allows researchers to effortlessly evaluate any algorithms on any existing domains; (ii) MORL-Baselines, a collection of reliable and efficient implementations of state-of-the-art MORL algorithms, designed to provide a solid foundation for advancing research. Notably, all algorithms are inherently compatible with MO-Gymnasium; and (iii) a thorough and robust set of benchmark results and comparisons of MORL-Baselines algorithms, tested across various challenging MO-Gymnasium environments. These benchmarks were constructed to serve as guidelines for the research community, underscoring the properties, advantages, and limitations of each particular state-of-the-art method.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Felten+2023</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Felten*, Florian and Alegre*, Lucas N. and Now{\'e}, Ann and Bazzan, Ana L. C. and Talbi, El-Ghazali and Danoy, Gr{\'e}goire and {da Silva}, Bruno C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Toolkit for Reliable Benchmarking and Research in Multi-Objective Reinforcement Learning}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{36}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{New Orleans, USA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/gpi-ls.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="gpi-ls.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://dl.acm.org/conference/aamas" rel="external nofollow noopener" target="_blank">AAMAS</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="Alegre+2023" class="col-sm-8">
        <!-- Title -->
        <div class="title">Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization</div>
        <!-- Author -->
        <div class="author">
        

        <em>Lucas N. Alegre</em>, <a href="https://roijers.info/" rel="external nofollow noopener" target="_blank">Diederik M. Roijers</a>, <a href="https://scholar.google.com/citations?user=LH5QKbgAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Ann Nowé</a>, <a href="https://www.inf.ufrgs.br/~bazzan" rel="external nofollow noopener" target="_blank">Ana L. C. Bazzan</a>, and <a href="https://people.cs.umass.edu/~bsilva/" rel="external nofollow noopener" target="_blank">Bruno C. da Silva</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proc. of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS)</em>, 2023
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);">  </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://arxiv.org/abs/2301.07784" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/LucasAlegre/morl-baselines" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="/assets/pdf/gpi-ls.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Multi-objective reinforcement learning (MORL) algorithms tackle sequential decision problems where agents may have different preferences over (possibly conflicting) reward functions. Such algorithms often learn a set of policies (each optimized for a particular agent preference) that can later be used to solve problems with novel preferences. We introduce a novel algorithm that uses Generalized Policy Improvement (GPI) to define principled, formally-derived prioritization schemes that improve sample-efficient learning. They implement active-learning strategies by which the agent can (i) identify the most promising preferences/objectives to train on at each moment, to more rapidly solve a given MORL problem; and (ii) identify which previous experiences are most relevant when learning a policy for a particular agent preference, via a novel Dyna-style MORL method. We prove our algorithm is guaranteed to always converge to an optimal solution in a finite number of steps, or an ϵ-optimal solution (for a bounded ϵ) if the agent is limited and can only identify possibly sub-optimal policies. We also prove that our method monotonically improves the quality of its partial solutions while learning. Finally, we introduce a bound that characterizes the maximum utility loss (with respect to the optimal solution) incurred by the partial solutions computed by our method throughout learning. We empirically show that our method outperforms state-of-the-art MORL algorithms in challenging multi-objective tasks, both with discrete and continuous state spaces.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Alegre+2023</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alegre, Lucas N. and Roijers, Diederik M. and Now{\'e}, Ann and Bazzan, Ana L. C. and {da Silva}, Bruno C.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proc. of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/sfols.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="sfols.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://icml.cc" rel="external nofollow noopener" target="_blank">ICML</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="Alegre+2022" class="col-sm-8">
        <!-- Title -->
        <div class="title">Optimistic Linear Support and Successor Features as a Basis for Optimal Policy Transfer</div>
        <!-- Author -->
        <div class="author">
        

        <em>Lucas N. Alegre</em>, <a href="https://www.inf.ufrgs.br/~bazzan" rel="external nofollow noopener" target="_blank">Ana L. C. Bazzan</a>, and <a href="https://people.cs.umass.edu/~bsilva/" rel="external nofollow noopener" target="_blank">Bruno C. da Silva</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 39th International Conference on Machine Learning</em>, 2022
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);">  </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://proceedings.mlr.press/v162/alegre22a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/LucasAlegre/sfols" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="/assets/pdf/sfols.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In many real-world applications, reinforcement learning (RL) agents might have to solve multiple tasks, each one typically modeled via a reward function. If reward functions are expressed linearly, and the agent has previously learned a set of policies for different tasks, successor features (SFs) can be exploited to combine such policies and identify reasonable solutions for new problems. However, the identified solutions are not guaranteed to be optimal. We introduce a novel algorithm that addresses this limitation. It allows RL agents to combine existing policies and directly identify optimal policies for arbitrary new problems, without requiring any further interactions with the environment. We first show (under mild assumptions) that the transfer learning problem tackled by SFs is equivalent to the problem of learning to optimize multiple objectives in RL. We then introduce an SF-based extension of the Optimistic Linear Support algorithm to learn a set of policies whose SFs form a convex coverage set. We prove that policies in this set can be combined via generalized policy improvement to construct optimal behaviors for any new linearly-expressible tasks, without requiring any additional training samples. We empirically show that our method outperforms state-of-the-art competing algorithms both in discrete and continuous domains under value function approximation.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Alegre+2022</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Optimistic Linear Support and Successor Features as a Basis for Optimal Policy Transfer}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alegre, Lucas N. and Bazzan, Ana L. C. and {da Silva}, Bruno C.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 39th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{394--413}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{162}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/mbcd.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="mbcd.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://dl.acm.org/conference/aamas" rel="external nofollow noopener" target="_blank">AAMAS</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="Alegre+2021aamas" class="col-sm-8">
        <!-- Title -->
        <div class="title">Minimum-Delay Adaptation in Non-Stationary Reinforcement Learning via Online High-Confidence Change-Point Detection</div>
        <!-- Author -->
        <div class="author">
        

        <em>Lucas N. Alegre</em>, <a href="https://www.inf.ufrgs.br/~bazzan" rel="external nofollow noopener" target="_blank">Ana L. C. Bazzan</a>, and <a href="https://people.cs.umass.edu/~bsilva/" rel="external nofollow noopener" target="_blank">Bruno C. da Silva</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)</em>, 2021
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);"> Best Paper Award at LXAI Workshop @ ICML 2021 </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="http://www.ifaamas.org/Proceedings/aamas2021/pdfs/p97.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
            <a href="https://github.com/LucasAlegre/sfols" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-isbn="9781450383073"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Non-stationary environments are challenging for reinforcement learning algorithms. If the state transition and/or reward functions change based on latent factors, the agent is effectively tasked with optimizing a behavior that maximizes performance over a possibly infinite random sequence of Markov Decision Processes (MDPs), each of which drawn from some unknown distribution. We call each such MDP a context. Most related works make strong assumptions such as knowledge about the distribution over contexts, the existence of pre-training phases, or a priori knowledge about the number, sequence, or boundaries between contexts. We introduce an algorithm that efficiently learns policies in non-stationary environments. It analyzes a possibly infinite stream of data and computes, in real-time, high-confidence change-point detection statistics that reflect whether novel, specialized policies need to be created and deployed to tackle novel contexts, or whether previously-optimized ones might be reused. We show that (i) this algorithm minimizes the delay until unforeseen changes to a context are detected, thereby allowing for rapid responses; and (ii) it bounds the rate of false alarm, which is important in order to minimize regret. Our method constructs a mixture model composed of a (possibly infinite) ensemble of probabilistic dynamics predictors that model the different modes of the distribution over underlying latent MDPs. We evaluate our algorithm on high-dimensional continuous reinforcement learning problems and show that it outperforms state-of-the-art (model-free and model-based) RL algorithms, as well as state-of-the-art meta-learning methods specially designed to deal with non-stationarity.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Alegre+2021aamas</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Minimum-Delay Adaptation in Non-Stationary Reinforcement Learning via Online High-Confidence Change-Point Detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alegre, Lucas N. and Bazzan, Ana L. C. and {da Silva}, Bruno C.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)}</span><span class="p">,</span>
  <span class="na">location</span> <span class="p">=</span> <span class="s">{Virtual Event, United Kingdom}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{97--105}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{9781450383073}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{International Foundation for Autonomous Agents and Multiagent Systems}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Richland, SC}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Best Paper Award at LXAI Workshop @ ICML 2021}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/its.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="its.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6979" rel="external nofollow noopener" target="_blank">IEEE ITS</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="Alegre+2021its" class="col-sm-8">
        <!-- Title -->
        <div class="title">Using Reinforcement Learning to Control Traffic Signals in a Real-World Scenario: an Approach Based on Linear Function Approximation</div>
        <!-- Author -->
        <div class="author">
        

        <em>Lucas N. Alegre</em>, <a href="https://www3.math.tu-berlin.de/coga/team/ziemke/" rel="external nofollow noopener" target="_blank">Theresa Ziemke</a>, and <a href="https://www.inf.ufrgs.br/~bazzan" rel="external nofollow noopener" target="_blank">Ana L. C. Bazzan</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>IEEE Transactions on Intelligent Transportation Systems</em>, 2021
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);">  </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/document/9468362" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/TITS.2021.3091014"></span>
              <span class="__dimensions_badge_embed__" data-doi="10.1109/TITS.2021.3091014" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Reinforcement learning is an efficient, widely used machine learning technique that performs well in problems with a reasonable number of states and actions. This is rarely the case regarding control-related problems, as for instance controlling traffic signals, where the state space can be very large. One way to deal with the curse of dimensionality is to use generalization techniques such as function approximation. In this paper, a linear function approximation is used by traffic signal agents in a network of signalized intersections. Specifically, a true online SARSA (λ) algorithm with Fourier basis functions (TOS(λ)-FB) is employed. This method has the advantage of having convergence guarantees and error bounds, a drawback of non-linear function approximation. In order to evaluate TOS(λ)-FB, we perform experiments in variations of an isolated intersection scenario and a scenario of the city of Cottbus, Germany, with 22 signalized intersections, implemented in MATSim. We compare our results not only to fixed-time controllers, but also to a state-of-the-art rule-based adaptive method, showing that TOS(λ)-FB shows a performance that is highly superior to the fixed-time, while also being at least as efficient as the rule-based approach. For more than half of the intersections, our approach leads to less congestion and delay, without the need for the knowledge that underlies the rule-based approach.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Alegre+2021its</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alegre, Lucas N. and Ziemke, Theresa and Bazzan, Ana L. C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Using Reinforcement Learning to Control Traffic Signals in a Real-World Scenario: an Approach Based on Linear Function Approximation}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Intelligent Transportation Systems}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TITS.2021.3091014}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/aicom.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="aicom.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://content.iospress.com/articles/ai-communications/aic201580" rel="external nofollow noopener" target="_blank">AI Communications</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="Ziemke+2021" class="col-sm-8">
        <!-- Title -->
        <div class="title">Reinforcement Learning vs. Rule-Based Adaptive Traffic Signal Control: A Fourier Basis Linear Function Approximation for Traffic Signal Control</div>
        <!-- Author -->
        <div class="author">
        

        <a href="https://www3.math.tu-berlin.de/coga/team/ziemke/" rel="external nofollow noopener" target="_blank">Theresa Ziemke</a>, <em>Lucas N. Alegre</em>, and <a href="https://www.inf.ufrgs.br/~bazzan" rel="external nofollow noopener" target="_blank">Ana L. C. Bazzan</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>AI Communications</em>, 2021
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);">  </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://content.iospress.com/articles/ai-communications/aic201580" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.3233/AIC-201580"></span>
              <span class="__dimensions_badge_embed__" data-doi="10.3233/AIC-201580" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Reinforcement learning is an efficient, widely used machine learning technique that performs well when the state and action spaces have a reasonable size. This is rarely the case regarding control-related problems, as for instance controlling traffic signals. Here, the state space can be very large. In order to deal with the curse of dimensionality, a rough discretization of such space can be employed. However, this is effective just up to a certain point. A way to mitigate this is to use techniques that generalize the state space such as function approximation. In this paper, a linear function approximation is used. Specifically, SARSA(λ) with Fourier basis features is implemented to control traffic signals in the agent-based transport simulation MATSim. The results are compared not only to trivial controllers such as fixed-time, but also to state-of-the-art rule-based adaptive methods. It is concluded that SARSA(λ) with Fourier basis features is able to outperform such methods, especially in scenarios with varying traffic demands or unexpected events.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Ziemke+2021</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ziemke, Theresa and Alegre, Lucas N. and Bazzan, Ana L. C.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reinforcement Learning vs. Rule-Based Adaptive Traffic Signal Control: A Fourier Basis Linear Function Approximation for Traffic Signal Control}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{AI Communications}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{89--103}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3233/AIC-201580}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/peerj.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="peerj.jpg" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://peerj.com/computer-science" rel="external nofollow noopener" target="_blank">PeerJ CS</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="Alegre+2021peerj" class="col-sm-8">
        <!-- Title -->
        <div class="title">Quantifying the Impact of Non-Stationarity in Reinforcement Learning-Based Traffic Signal Control</div>
        <!-- Author -->
        <div class="author">
        

        <em>Lucas N. Alegre</em>, <a href="https://www.inf.ufrgs.br/~bazzan" rel="external nofollow noopener" target="_blank">Ana L. C. Bazzan</a>, and <a href="https://people.cs.umass.edu/~bsilva/" rel="external nofollow noopener" target="_blank">Bruno C. da Silva</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>PeerJ Computer Science</em>, 2021
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);">  </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://peerj.com/articles/cs-575/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.7717/peerj-cs.575"></span>
              <span class="__dimensions_badge_embed__" data-doi="10.7717/peerj-cs.575" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In reinforcement learning (RL), dealing with non-stationarity is a challenging issue. However, some domains such as traffic optimization are inherently non-stationary. Causes for and effects of this are manifold. In particular, when dealing with traffic signal controls, addressing non-stationarity is key since traffic conditions change over time and as a function of traffic control decisions taken in other parts of a network. In this paper we analyze the effects that different sources of non-stationarity have in a network of traffic signals, in which each signal is modeled as a learning agent. More precisely, we study both the effects of changing the context in which an agent learns (e.g., a change in flow rates experienced by it), as well as the effects of reducing agent observability of the true environment state. Partial observability may cause distinct states (in which distinct actions are optimal) to be seen as the same by the traffic signal agents. This, in turn, may lead to sub-optimal performance. We show that the lack of suitable sensors to provide a representative observation of the real state seems to affect the performance more drastically than the changes to the underlying traffic patterns.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Alegre+2021peerj</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Quantifying the Impact of Non-Stationarity in Reinforcement Learning-Based Traffic Signal Control}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alegre, Lucas N. and Bazzan, Ana L. C. and {da Silva}, Bruno C.}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{2376-5992}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://dx.doi.org/10.7717/peerj-cs.575}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.7717/peerj-cs.575}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{PeerJ Computer Science}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PeerJ}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{e575}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/selfieart.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="selfieart.gif" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://ieeexplore.ieee.org/xpl/conhome/9265968/proceeding" rel="external nofollow noopener" target="_blank">SIBGRAPI</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="AlegreOliveira2020" class="col-sm-8">
        <!-- Title -->
        <div class="title">SelfieArt: Interactive Multi-Style Transfer for Selfies and Videos with Soft Transitions</div>
        <!-- Author -->
        <div class="author">
        

        <em>Lucas N. Alegre</em>, and <a href="https://www.inf.ufrgs.br/~oliveira/" rel="external nofollow noopener" target="_blank">Manuel M. Oliveira</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images</em>, 2020
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);">  </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://ieeexplore.ieee.org/document/9266016" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="10.1109/SIBGRAPI51738.2020.00011"></span>
              <span class="__dimensions_badge_embed__" data-doi="10.1109/SIBGRAPI51738.2020.00011" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We introduce SelfieArt, an interactive technique for performing multi-style transfer for portraits and videos. Our method provides a simple and intuitive way of producing exquisite artistic results that combine multiple styles in a harmonious fashion. It uses face parsing and a multi-style transfer model to apply different styles to the various semantic segments. This is achieved using parameterized soft masks, allowing users to adjust the smoothness of the transitions between stylized regions in real-time. We demonstrate the effectiveness of our solution on a large set of images and videos. Given its flexibility, speed, and quality of results, our solution can be a valuable tool for creative exploration, allowing anyone to transform photographs and drawings in world-class artistic results.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">AlegreOliveira2020</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Alegre, Lucas N. and Oliveira, Manuel M.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SelfieArt: Interactive Multi-Style Transfer for Selfies and Videos with Soft Transitions}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{17--22}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/SIBGRAPI51738.2020.00011}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 preview">
<figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/publication_preview/nime.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="nime.png" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>
<abbr class="badge"><a href="https://www.nime.org" rel="external nofollow noopener" target="_blank">NIME</a></abbr>
</div>

        <!-- Entry bib key -->
        <div id="weber+2019" class="col-sm-8">
        <!-- Title -->
        <div class="title">Parameterized Melody Generation with Autoencoders and Temporally-Consistent Noise</div>
        <!-- Author -->
        <div class="author">
        

        Aline Weber, <em>Lucas N. Alegre</em>, <a href="https://jimtoer.no/" rel="external nofollow noopener" target="_blank">Jim Torresen</a>, and <a href="https://people.cs.umass.edu/~bsilva/" rel="external nofollow noopener" target="_blank">Bruno C. da Silva</a>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the International Conference on New Interfaces for Musical Expression</em>, 2019
        </div>
        <div class="periodical">
          <b style="color:rgb(179, 128, 34);">  </b>
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://www.nime.org/proceedings/2019/nime2019_paper035.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We introduce a machine learning technique to autonomously generate novel melodies that are variations of an arbitrary base melody. These are produced by a neural network that ensures that (with high probability) the melodic and rhythmic structure of the new melody is consistent with a given set of sample songs. We train a Variational Autoencoder network to identify a low-dimensional set of variables that allows for the compression and representation of sample songs. By perturbing these variables with Perlin Noise—a temporally-consistent parameterized noise function—it is possible to generate smoothly-changing novel melodies. We show that (1) by regulating the amount of noise, one can specify how much of the base song will be preserved; and (2) there is a direct correlation between the noise signal and the differences between the statistical properties of novel melodies and the original one. Users can interpret the controllable noise as a type of "creativity knob": the higher it is, the more leeway the network has to generate significantly different melodies. We present a physical prototype that allows musicians to use a keyboard to provide base melodies and to adjust the network’s "creativity knobs" to regulate in real-time the process that proposes new melody ideas.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">weber+2019</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Weber, Aline and Alegre, Lucas N. and Torresen, Jim and {da Silva}, Bruno C.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the International Conference on New Interfaces for Musical Expression}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>


          In my free time, I also like to play guitar! - <a href="https://www.youtube.com/watch?v=e_QFfHr2ULw" rel="external nofollow noopener" target="_blank">Pink Floyd - Money (Guitar Solo Cover)</a>
          <br>

          <!-- Social -->
            <div class="social">
              <br>
              <div class="contact-icons">
                <a href="mailto:%6C%6E%61%6C%65%67%72%65@%69%6E%66.%75%66%72%67%73.%62%72" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0001-5465-4390" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="fab fa-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=YZnEeJUAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="http://lattes.cnpq.br/8350276337206370" target="_blank" title="Lattes" rel="external nofollow noopener"><i class="ai ai-lattes"></i></a>
            <a href="https://github.com/LucasAlegre" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/lucas-alegre-b80628127" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://bsky.app/profile/lnalegre.bsky.social" title="BlueSky" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-bluesky"></i></a>
            <a href="https://twitter.com/lnalegre" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a>
            

              </div>

              <div class="contact-note">
                
              </div>

            </div>
        </article>

</div>

      
    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2025 Lucas N. Alegre. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script><!-- Panelbear Analytics - We respect your privacy -->
  <script async src="https://cdn.panelbear.com/analytics.js?site="></script>
  <script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: '' });
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
