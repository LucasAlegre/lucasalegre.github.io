---
---

@InProceedings{Alegre+2023,
  title = {Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization},
  author = {Lucas N. Alegre and Diederik M. Roijers and Ann Now{\'e} and Ana L. C. Bazzan and Bruno C. {da Silva}},
  booktitle = {Proc. of the 22nd International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
  year = {2023},
  preview = {gpi-ls.png},
  entrytype = {inproceedings},
  abbr = {AAMAS},
  pdf = {https://arxiv.org/abs/2301.07784},
  poster = {gpi-ls.pdf},
  bibtex_show = {true},
  selected = {true},
  code = {https://github.com/LucasAlegre/morl-baselines},
  abstract = {Multi-objective reinforcement learning (MORL) algorithms tackle sequential decision problems where agents may have different preferences over (possibly conflicting) reward functions. Such algorithms often learn a set of policies (each optimized for a particular agent preference) that can later be used to solve problems with novel preferences. We introduce a novel algorithm that uses Generalized Policy Improvement (GPI) to define principled, formally-derived prioritization schemes that improve sample-efficient learning. They implement active-learning strategies by which the agent can (i) identify the most promising preferences/objectives to train on at each moment, to more rapidly solve a given MORL problem; and (ii) identify which previous experiences are most relevant when learning a policy for a particular agent preference, via a novel Dyna-style MORL method. We prove our algorithm is guaranteed to always converge to an optimal solution in a finite number of steps, or an ϵ-optimal solution (for a bounded ϵ) if the agent is limited and can only identify possibly sub-optimal policies. We also prove that our method monotonically improves the quality of its partial solutions while learning. Finally, we introduce a bound that characterizes the maximum utility loss (with respect to the optimal solution) incurred by the partial solutions computed by our method throughout learning. We empirically show that our method outperforms state-of-the-art MORL algorithms in challenging multi-objective tasks, both with discrete and continuous state spaces.}
}

@inproceedings{Thomasini+2023,
  author          = {Luiz A. Thomasini and Lucas N. Alegre and Gabriel O. Ramos and Ana L. C. Bazzan},
  booktitle       = {Adaptive and Learning Agents Workshop at AAMAS},
  title           = {RouteChoiceEnv: a Route Choice Library for Multiagent Reinforcement Learning},
  year            = {2023},
  preview = {routechoice.png},
  entrytype = {inproceedings},
  abbr = {ALA},
  pdf = {https://alaworkshop2023.github.io/papers/ALA2023_paper_69.pdf},
  bibtex_show = {true},
  selected = {false},
  code = {https://github.com/ramos-ai/route-choice-env},
  abstract = {Multiagent Reinforcement Learning (MARL) has been successfully applied as a framework for solving distributed traffic optimization problems. Route choice is a challenging traffic problem in which driver agents must select routes that minimize their own travel times, taking into account the effect caused by other drivers. While MARL algorithms for route choice have been proposed, there is no library that provides a set of benchmarks and algorithms that can be used by researchers in the field. In this paper, we fill this gap by introducing Route Choice Env, a centralized library for MARLbased route choice research. It follows the PettingZoo API, which allows us to provide a standard set of environments and agents for reproducible experimentation. The library is publicly available at https://github.com/ramos-ai/route-choice-env.}
}

@InProceedings{Alegre+2022,
  title = 	 {Optimistic Linear Support and Successor Features as a Basis for Optimal Policy Transfer},
  author = {Lucas N. Alegre and Ana L. C. Bazzan and Bruno C. {da Silva}},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {394--413},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  url = {https://proceedings.mlr.press/v162/alegre22a.html},
  entrytype = {inproceedings},
  abbr = {ICML},
  bibtex_show = {true},
  selected = {true},
  code = {https://github.com/LucasAlegre/sfols},
  preview = {sfols.png},
  poster = {sfols.pdf},
  pdf = {https://proceedings.mlr.press/v162/alegre22a.html},
  abstract = {In many real-world applications, reinforcement learning (RL) agents might have to solve multiple tasks, each one typically modeled via a reward function. If reward functions are expressed linearly, and the agent has previously learned a set of policies for different tasks, successor features (SFs) can be exploited to combine such policies and identify reasonable solutions for new problems. However, the identified solutions are not guaranteed to be optimal. We introduce a novel algorithm that addresses this limitation. It allows RL agents to combine existing policies and directly identify optimal policies for arbitrary new problems, without requiring any further interactions with the environment. We first show (under mild assumptions) that the transfer learning problem tackled by SFs is equivalent to the problem of learning to optimize multiple objectives in RL. We then introduce an SF-based extension of the Optimistic Linear Support algorithm to learn a set of policies whose SFs form a convex coverage set. We prove that policies in this set can be combined via generalized policy improvement to construct optimal behaviors for any new linearly-expressible tasks, without requiring any additional training samples. We empirically show that our method outperforms state-of-the-art competing algorithms both in discrete and continuous domains under value function approximation.}
}

@InProceedings{Alegre+2021aamas,
  title		= {Minimum-Delay Adaptation in Non-Stationary Reinforcement Learning via Online High-Confidence Change-Point Detection},
  author	= {Lucas N. Alegre and Ana L. C. Bazzan and Bruno C. {da Silva}},
  booktitle	= {Proceedings of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
  location = {Virtual Event, United Kingdom},
  year		= {2021},
  pages = {97--105},
  isbn = {9781450383073},
  publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
  address = {Richland, SC},
  entrytype = {inproceedings},
  abbr = {AAMAS},
  bibtex_show = {true},
  selected = {true},
  code = {https://github.com/LucasAlegre/sfols},
  preview = {mbcd.gif},
  pdf = {http://www.ifaamas.org/Proceedings/aamas2021/pdfs/p97.pdf},
  abstract = {Non-stationary environments are challenging for reinforcement learning algorithms. If the state transition and/or reward functions change based on latent factors, the agent is effectively tasked with optimizing a behavior that maximizes performance over a possibly infinite random sequence of Markov Decision Processes (MDPs), each of which drawn from some unknown distribution. We call each such MDP a context. Most related works make strong assumptions such as knowledge about the distribution over contexts, the existence of pre-training phases, or a priori knowledge about the number, sequence, or boundaries between contexts. We introduce an algorithm that efficiently learns policies in non-stationary environments. It analyzes a possibly infinite stream of data and computes, in real-time, high-confidence change-point detection statistics that reflect whether novel, specialized policies need to be created and deployed to tackle novel contexts, or whether previously-optimized ones might be reused. We show that (i) this algorithm minimizes the delay until unforeseen changes to a context are detected, thereby allowing for rapid responses; and (ii) it bounds the rate of false alarm, which is important in order to minimize regret. Our method constructs a mixture model composed of a (possibly infinite) ensemble of probabilistic dynamics predictors that model the different modes of the distribution over underlying latent MDPs. We evaluate our algorithm on high-dimensional continuous reinforcement learning problems and show that it outperforms state-of-the-art (model-free and model-based) RL algorithms, as well as state-of-the-art meta-learning methods specially designed to deal with non-stationarity.},
  note = {Best Paper Award at LXAI Workshop @ ICML 2021}
}

@inproceedings{Alegre+2022bnaic,
  author = {Lucas N. Alegre and Florian Felten and El-Ghazali Talbi and Gr{\'e}goire Danoy and Ann Now{\'e} and Ana L. C. Bazzan and Bruno C. {da Silva}},
  title = {{MO-Gym}: A Library of Multi-Objective Reinforcement Learning Environments},
  booktitle = {Proceedings of the 34th Benelux Conference on Artificial Intelligence BNAIC/Benelearn 2022},
  year = {2022},
  entrytype = {inproceedings},
  abbr = {BNAIC},
  bibtex_show = {true},
  selected = {true},
  code = {https://github.com/Farama-Foundation/MO-Gymnasium},
  preview = {mo-gym-logo.png},
  pdf = {https://people.cs.umass.edu/~bsilva/papers/MO-Gym_BNAIC_2022.pdf},
  abstract = {We introduce MO-Gym, an extensible library containing a diverse set of multi-objective reinforcement learning environments. It introduces a standardized API that facilitates conducting experiments and performance analyses of algorithms designed to interact with multiobjective Markov decision processes. Importantly, it extends the widelyused OpenAI Gym API, allowing the reuse of algorithms and features that are well-established in the reinforcement learning community. MOGym is available at: https://github.com/LucasAlegre/mo-gym.}
}

@Article{Alegre+2021its,
  author	= {Lucas N. Alegre and Theresa Ziemke and Ana L. C. Bazzan},
  title		= {Using Reinforcement Learning to Control Traffic Signals in a Real-World Scenario: an Approach Based on Linear Function Approximation},
  journal	= {IEEE Transactions on Intelligent Transportation Systems},
  pages = {},
  doi = {10.1109/TITS.2021.3091014},
  year		= {2021},
  entrytype = {journal},
  abbr = {IEEE ITS},
  bibtex_show = {true},
  selected = {true},
  preview = {its.png},
  pdf = {https://ieeexplore.ieee.org/document/9468362},
  abstract = {Reinforcement learning is an efficient, widely used machine learning technique that performs well in problems with a reasonable number of states and actions. This is rarely the case regarding control-related problems, as for instance controlling traffic signals, where the state space can be very large. One way to deal with the curse of dimensionality is to use generalization techniques such as function approximation. In this paper, a linear function approximation is used by traffic signal agents in a network of signalized intersections. Specifically, a true online SARSA (λ) algorithm with Fourier basis functions (TOS(λ)-FB) is employed. This method has the advantage of having convergence guarantees and error bounds, a drawback of non-linear function approximation. In order to evaluate TOS(λ)-FB, we perform experiments in variations of an isolated intersection scenario and a scenario of the city of Cottbus, Germany, with 22 signalized intersections, implemented in MATSim. We compare our results not only to fixed-time controllers, but also to a state-of-the-art rule-based adaptive method, showing that TOS(λ)-FB shows a performance that is highly superior to the fixed-time, while also being at least as efficient as the rule-based approach. For more than half of the intersections, our approach leads to less congestion and delay, without the need for the knowledge that underlies the rule-based approach.}
}

@InProceedings{AlegreOliveira2020,
  author    = {Alegre, Lucas N. and Oliveira, Manuel M.},
  title     = {SelfieArt: Interactive Multi-Style Transfer for Selfies and Videos with Soft Transitions},
  booktitle = {Proceedings of the 2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images},
  year      = {2020},
  pages     = {17--22},
  doi       = {10.1109/SIBGRAPI51738.2020.00011},
  entrytype = {inproceedings},
  abbr = {SIBGRAPI},
  bibtex_show = {true},
  selected = {true},
  preview = {selfieart.gif},
  pdf = {https://ieeexplore.ieee.org/document/9266016},
  abstract = {We introduce SelfieArt, an interactive technique for performing multi-style transfer for portraits and videos. Our method provides a simple and intuitive way of producing exquisite artistic results that combine multiple styles in a harmonious fashion. It uses face parsing and a multi-style transfer model to apply different styles to the various semantic segments. This is achieved using parameterized soft masks, allowing users to adjust the smoothness of the transitions between stylized regions in real-time. We demonstrate the effectiveness of our solution on a large set of images and videos. Given its flexibility, speed, and quality of results, our solution can be a valuable tool for creative exploration, allowing anyone to transform photographs and drawings in world-class artistic results.}
}

@Article{Ziemke+2021,
    author	= {Theresa Ziemke and Lucas N. Alegre and Ana L. C. Bazzan},
    title		= {Reinforcement Learning vs. Rule-Based Adaptive Traffic Signal Control: A Fourier Basis Linear Function Approximation for Traffic Signal Control},
    journal	= {AI Communications},
    year		= {2021},
    pages     = {89--103},
    doi		= {10.3233/AIC-201580},
    entrytype = {journal},
    abbr = {AI Communications},
    bibtex_show = {true},
    selected = {true},
    preview = {aicom.png},
    pdf = {https://content.iospress.com/articles/ai-communications/aic201580},
    abstract = {Reinforcement learning is an efficient, widely used machine learning technique that performs well when the state and action spaces have a reasonable size. This is rarely the case regarding control-related problems, as for instance controlling traffic signals. Here, the state space can be very large. In order to deal with the curse of dimensionality, a rough discretization of such space can be employed. However, this is effective just up to a certain point. A way to mitigate this is to use techniques that generalize the state space such as function approximation. In this paper, a linear function approximation is used. Specifically, SARSA(λ) with Fourier basis features is implemented to control traffic signals in the agent-based transport simulation MATSim. The results are compared not only to trivial controllers such as fixed-time, but also to state-of-the-art rule-based adaptive methods. It is concluded that SARSA(λ) with Fourier basis features is able to outperform such methods, especially in scenarios with varying traffic demands or unexpected events.}
}

@article{Alegre+2021peerj,
  title={Quantifying the Impact of Non-Stationarity in Reinforcement Learning-Based Traffic Signal Control},
  author={Lucas N. Alegre and Ana L. C. Bazzan and Bruno C. {da Silva}},
  volume={7},
  ISSN={2376-5992},
  url={http://dx.doi.org/10.7717/peerj-cs.575},
  DOI={10.7717/peerj-cs.575},
  journal={PeerJ Computer Science},
  publisher={PeerJ},
  year={2021},
  pages={e575},
  entrytype = {journal},
  abbr = {PeerJ CS},
  bibtex_show = {true},
  selected = {true},
  preview = {peerj.jpg},
  pdf = {https://peerj.com/articles/cs-575/},
  abstract = {In reinforcement learning (RL), dealing with non-stationarity is a challenging issue. However, some domains such as traffic optimization are inherently non-stationary. Causes for and effects of this are manifold. In particular, when dealing with traffic signal controls, addressing non-stationarity is key since traffic conditions change over time and as a function of traffic control decisions taken in other parts of a network. In this paper we analyze the effects that different sources of non-stationarity have in a network of traffic signals, in which each signal is modeled as a learning agent. More precisely, we study both the effects of changing the context in which an agent learns (e.g., a change in flow rates experienced by it), as well as the effects of reducing agent observability of the true environment state. Partial observability may cause distinct states (in which distinct actions are optimal) to be seen as the same by the traffic signal agents. This, in turn, may lead to sub-optimal performance. We show that the lack of suitable sensors to provide a representative observation of the real state seems to affect the performance more drastically than the changes to the underlying traffic patterns.}
}

@INPROCEEDINGS{Schreiber+2022,
  author={Schreiber, Lincoln V. and Alegre, Lucas N. and Bazzan, Ana L. C. and Ramos, Gabriel O.},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)}, 
  title={On the Explainability and Expressiveness of Function Approximation Methods in RL-Based Traffic Signal Control}, 
  year={2022},
  pages={01--08},
  doi={10.1109/IJCNN55064.2022.9892422},
  entrytype = {inproceedings},
  abbr = {IJCNN},
  bibtex_show = {true},
  selected = {false},
  preview = {xaidqn.png},
  pdf = {https://ieeexplore.ieee.org/abstract/document/9892422/},
  abstract = {With fast increasing urbanization levels, adaptive traffic signal control methods have great potential for optimizing traffic jams. In particular, deep reinforcement learning (RL) approaches have been shown to be able to outperform classic control methods. However, deep RL algorithms are often employed as black boxes, which limits their use in the real-world as the decisions made by the agents can not be properly explained. In this paper, we compare different function approximations methods used to estimate de action-value function of RL-based traffic controllers. In particular, we compare (i) their expressiveness, based on the resulting performance of the learned policies, and (ii) their explainability capabilities. To explain the decisions of each method, we use Shapley Additive Explanations (SHAP) to show the impact of the agent's state features on each possible action. This allows us to explain the learned policies with a single image, enabling an understanding of how the agent behaves in the face of different traffic conditions. In addition, we discuss the application of post-hoc explainability models in the context of adaptive traffic signal control, noting their potential and pointing out some of their limitations. Comparing our resulting methods to state-of-the-art adaptive traffic signal controllers, we saw significant improvements in travel time, speed score, and throughput in two different scenarios based on real traffic data.}
}

@inproceedings{weber+2019,
  Author    = {Aline Weber and Lucas N. Alegre and Jim Torresen and Bruno C. {da Silva}},
  Title     = {Parameterized Melody Generation with Autoencoders and Temporally-Consistent Noise},
  Booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  Year      = {2019},
  entrytype = {inproceedings},
  abbr = {NIME},
  bibtex_show = {true},
  selected = {true},
  preview = {nime.png},
  pdf = {https://www.nime.org/proceedings/2019/nime2019_paper035.pdf},
  abstract = {We introduce a machine learning technique to autonomously generate novel melodies that are variations of an arbitrary base melody. These are produced by a neural network that ensures that (with high probability) the melodic and rhythmic structure of the new melody is consistent with a given set of sample songs. We train a Variational Autoencoder network to identify a low-dimensional set of variables that allows for the compression and representation of sample songs. By perturbing these variables with Perlin Noise---a temporally-consistent parameterized noise function---it is possible to generate smoothly-changing novel melodies. We show that (1) by regulating the amount of noise, one can specify how much of the base song will be preserved; and (2) there is a direct correlation between the noise signal and the differences between the statistical properties of novel melodies and the original one. Users can interpret the controllable noise as a type of "creativity knob": the higher it is, the more leeway the network has to generate significantly different melodies. We present a physical prototype that allows musicians to use a keyboard to provide base melodies and to adjust the network's "creativity knobs" to regulate in real-time the process that proposes new melody ideas.}
}